{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "833a6664",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-5-07d7c37826b1>, line 128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-07d7c37826b1>\"\u001b[0;36m, line \u001b[0;32m128\u001b[0m\n\u001b[0;31m    candidate, references = fetch_data('c.txt', 'r.txt)\u001b[0m\n\u001b[0m                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#-*- coding:utf-8 -*-\n",
    "import sys\n",
    "import codecs\n",
    "import os\n",
    "import math\n",
    "import operator\n",
    "import json\n",
    "\n",
    "\n",
    "# 如果是一份答案的话，务必在答案的后面加上.txt   python Bleu.py Candidate ref.txt \n",
    "# 如果是多份答案的话，把多份答案放到一个文件夹中  python Bleu.py Candidate 文件夹\n",
    "\n",
    "def fetch_data(cand, ref):\n",
    "    \"\"\" Store each reference and candidate sentences as a list \"\"\"\n",
    "    references = []\n",
    "    if '.txt' in ref:\n",
    "        reference_file = codecs.open(ref, 'r', 'utf-8')\n",
    "        references.append(reference_file.readlines())\n",
    "    else:\n",
    "        for root, dirs, files in os.walk(ref):\n",
    "            for f in files:\n",
    "                reference_file = codecs.open(os.path.join(root, f), 'r', 'utf-8')\n",
    "                references.append(reference_file.readlines())\n",
    "    candidate_file = codecs.open(cand, 'r', 'utf-8')\n",
    "    candidate = candidate_file.readlines()\n",
    "    return candidate, references\n",
    "\n",
    "\n",
    "def count_ngram(candidate, references, n):\n",
    "    clipped_count = 0\n",
    "    count = 0\n",
    "    r = 0\n",
    "    c = 0\n",
    "    for si in range(len(candidate)):\n",
    "        # Calculate precision for each sentence\n",
    "        #print si\n",
    "        ref_counts = []\n",
    "        ref_lengths = []\n",
    "        #print references\n",
    "        # Build dictionary of ngram counts\n",
    "        for reference in references:\n",
    "            #print 'reference' + reference\n",
    "            ref_sentence = reference[si]\n",
    "            ngram_d = {}\n",
    "            words = ref_sentence.strip().split()\n",
    "            ref_lengths.append(len(words))\n",
    "            limits = len(words) - n + 1\n",
    "            # loop through the sentance consider the ngram length\n",
    "            for i in range(limits):\n",
    "                ngram = ' '.join(words[i:i+n]).lower()\n",
    "                if ngram in ngram_d.keys():\n",
    "                    ngram_d[ngram] += 1\n",
    "                else:\n",
    "                    ngram_d[ngram] = 1\n",
    "            ref_counts.append(ngram_d)\n",
    "        # candidate\n",
    "        cand_sentence = candidate[si]\n",
    "        cand_dict = {}\n",
    "        words = cand_sentence.strip().split()\n",
    "        limits = len(words) - n + 1\n",
    "        for i in range(0, limits):\n",
    "            ngram = ' '.join(words[i:i + n]).lower()\n",
    "            if ngram in cand_dict:\n",
    "                cand_dict[ngram] += 1\n",
    "            else:\n",
    "                cand_dict[ngram] = 1\n",
    "        clipped_count += clip_count(cand_dict, ref_counts)\n",
    "        count += limits\n",
    "        r += best_length_match(ref_lengths, len(words))\n",
    "        c += len(words)\n",
    "    if clipped_count == 0:\n",
    "        pr = 0\n",
    "    else:\n",
    "        pr = float(clipped_count) / count\n",
    "    bp = brevity_penalty(c, r)\n",
    "    return pr, bp\n",
    "\n",
    "\n",
    "def clip_count(cand_d, ref_ds):\n",
    "    \"\"\"Count the clip count for each ngram considering all references\"\"\"\n",
    "    count = 0\n",
    "    for m in cand_d.keys():\n",
    "        m_w = cand_d[m]\n",
    "        m_max = 0\n",
    "        for ref in ref_ds:\n",
    "            if m in ref:\n",
    "                m_max = max(m_max, ref[m])\n",
    "        m_w = min(m_w, m_max)\n",
    "        count += m_w\n",
    "    return count\n",
    "\n",
    "\n",
    "def best_length_match(ref_l, cand_l):\n",
    "    \"\"\"Find the closest length of reference to that of candidate\"\"\"\n",
    "    least_diff = abs(cand_l-ref_l[0])\n",
    "    best = ref_l[0]\n",
    "    for ref in ref_l:\n",
    "        if abs(cand_l-ref) < least_diff:\n",
    "            least_diff = abs(cand_l-ref)\n",
    "            best = ref\n",
    "    return best\n",
    "\n",
    "\n",
    "def brevity_penalty(c, r):\n",
    "    if c > r:\n",
    "        bp = 1\n",
    "    else:\n",
    "        bp = math.exp(1-(float(r)/c))\n",
    "    \n",
    "    return bp\n",
    "\n",
    "\n",
    "def geometric_mean(precisions):\n",
    "    return (reduce(operator.mul, precisions)) ** (1.0 / len(precisions))\n",
    "\n",
    "\n",
    "def BLEU(candidate, references):\n",
    "    precisions = []\n",
    "    for i in range(4):\n",
    "        pr, bp = count_ngram(candidate, references, i+1)\n",
    "        precisions.append(pr)\n",
    "        #print 'P'+str(i+1), ' = ',round(pr, 2)\n",
    "    print('BP = ',round(bp, 2)) \n",
    "    bleu = geometric_mean(precisions) * bp\n",
    "    return bleu\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    candidate, references = fetch_data('c.txt', 'r.txt)\n",
    "    bleu = BLEU(candidate, references)\n",
    "    print 'BLEU = ',round(bleu, 4)\n",
    "    out = open('bleu_out.txt', 'w')\n",
    "    out.write(str(bleu))\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "944522eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The number of hypotheses and their reference(s) should be the same ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-bd0ef5ab6901>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mreferences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcandidates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'this'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.7/site-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     assert len(list_of_references) == len(hypotheses), (\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0;34m\"The number of hypotheses and their reference(s) should be the \"\u001b[0m \u001b[0;34m\"same \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m     )\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The number of hypotheses and their reference(s) should be the same "
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "references = [['this', 'is', 'a', 'test'],['this', 'is','test']]\n",
    "candidates = [['this', 'is', 'a', 'test']]\n",
    "score = corpus_bleu(references, candidates)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58098085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "# 不加Here的话结果是1/7，即无法把大写的The与the进行匹配，该问题后面再解决\n",
    "reference1 = \"我说：请大家多多关照\".split()\n",
    "reference = [reference1]\n",
    "candidate = \"请大家多多关照。\".split()\n",
    "score = sentence_bleu(reference, candidate, weights=[1, 0, 0, 0])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9ac298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
